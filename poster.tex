\documentclass{ctexbeamer}
\usepackage[T1]{fontenc}
\usepackage[size=custom, width=91.44, height=60.94, scale=1.0]{beamerposter}

% Theme and color theme
\usetheme{nju}
\usecolortheme{njucolor}

% Required packages
\usepackage{graphicx,lmodern,booktabs,tikz,pgfplots,amsfonts,amsmath,amssymb,amsthm,hyperref,caption,newtxmath}

% Tikz Customization
\usetikzlibrary{intersections}
\usepgfplotslibrary{fillbetween}

% PGFPlots config
\pgfplotsset{compat=1.14}

% Dimensions for column layout
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.009\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% Title and authors
\title{Mathematical Foundations of Artificial Intelligence}
\author{Eric Li \inst{1} \and AI Research Team \inst{1}}
\institute[shortinst]{\inst{1} School of Artificial Intelligence, Nanjing University}

% Footer content
\footercontent{
\href{https://github.com/ericli2333/NJU-beamer-theme-template}{github.com/ericli2333/NJU-beamer-theme-template} \hfill Modified from FSU Template\hfill Nanjing University AI School
}

% Logos (replace with your real paths)
\logoleft{\includegraphics[height=6.5cm]{logo/NJU-gold.png}}
\logoright{\includegraphics[height=7cm]{logo/NJU-gold.png}}

\begin{document}

\begin{frame}[t]
\begin{columns}[t]

% COLUMN 1
\begin{column}{\colwidth}
\begin{block}{Introduction}
This poster template is modified from the original FSU Mathematics template by Rafiq Islam\cite{fsumathposter25} for Nanjing University AI School presentations\cite{njumathposter25}.\\
Artificial Intelligence encompasses the development of systems that can perform tasks requiring human intelligence, including learning, reasoning, perception, and decision-making.
\end{block}

\begin{block}{Mathematical Foundations}
    The core of modern AI lies in mathematical optimization and linear algebra. The fundamental neural network forward propagation is:
        \[
        a^{(l)} = \sigma(W^{(l)}a^{(l-1)} + b^{(l)})
        \]
    where $\sigma$ is the activation function, $W^{(l)}$ are weight matrices, and $b^{(l)}$ are bias vectors.

    Backpropagation uses gradient descent to minimize the loss function:
        \[
        \theta \leftarrow \theta - \eta \nabla_\theta J(\theta)
        \]
\end{block}

\begin{block}{Neural Network Architecture}
    \begin{figure}
        \centering
        \begin{tikzpicture}[scale=3]
            % Input layer
            \foreach \i in {1,...,3}
                \node[circle,draw,minimum size=0.5cm] (input\i) at (0,-\i) {$x_\i$};
            
            % Hidden layer
            \foreach \i in {1,...,4}
                \node[circle,draw,minimum size=0.5cm] (hidden\i) at (2,-\i+0.5) {$h_\i$};
            
            % Output layer
            \node[circle,draw,minimum size=0.5cm] (output) at (4,-2) {$y$};
            
            % Connections
            \foreach \i in {1,...,3}
                \foreach \j in {1,...,4}
                    \draw[->] (input\i) -- (hidden\j);
            
            \foreach \i in {1,...,4}
                \draw[->] (hidden\i) -- (output);
            
            % Labels
            \node[above] at (0,0) {Input};
            \node[above] at (2,0) {Hidden};
            \node[above] at (4,0) {Output};
        \end{tikzpicture}
        \caption{Basic Neural Network Architecture}
        \label{fig:nn_arch}
    \end{figure}
\end{block}

\begin{block}{Research Objectives}
\begin{itemize}
  \item Develop advanced deep learning architectures
  \item Improve natural language understanding capabilities
  \item Enhance computer vision systems for real-world applications
  \item 推动人工智能理论创新 (Advancing AI theoretical innovation)
\end{itemize}
\end{block}

\begin{alertblock}{Why Artificial Intelligence?}
Artificial Intelligence is revolutionizing multiple industries and creating new possibilities:
\begin{itemize}
  \item Healthcare: Early disease detection and personalized treatment
  \item Transportation: Autonomous vehicles and smart traffic systems
  \item Education: Adaptive learning platforms and intelligent tutoring
  \item Finance: Risk assessment and algorithmic trading
\end{itemize}
\end{alertblock}
\end{column}

\separatorcolumn

% COLUMN 2
\begin{column}{\colwidth}
\begin{block}{Methodology}
Our research methodology follows a systematic approach:\hfill
\begin{enumerate}
  \item Problem formulation and mathematical modeling
  \item Data collection and preprocessing
  \item Algorithm design and implementation
  \item Experimental evaluation and performance analysis
  \item Theoretical analysis and generalization studies
\end{enumerate}
\end{block}

\begin{block}{AI Research Areas}
\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{images/lamda.png}
    \caption{NJU LAMDA Research Team (Photo credit: \href{https://lamda.nju.edu.cn/}{LAMDA})}
    \label{fig:ai_domains}
\end{figure}
\begin{itemize}
  \item Machine Learning and Deep Learning
  \item Natural Language Processing
  \item Computer Vision and Robotics
  \item Knowledge Representation and Reasoning
\end{itemize}
\end{block}

\begin{block}{Performance Comparison}
    \begin{center}
        \renewcommand{\arraystretch}{1.4}
        \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} \\
        \hline
        CNN & 94.2\% & 93.8\% & 94.5\% \\
        RNN & 91.5\% & 90.2\% & 92.1\% \\
        Transformer & 96.8\% & 96.5\% & 97.1\% \\
        Traditional ML & 87.3\% & 86.1\% & 88.2\% \\
        \hline
        \end{tabular}
        \vspace{0.5em}
        \captionof{table}{Performance comparison of different AI models on benchmark datasets}
    \end{center}
\end{block}

\end{column}


\separatorcolumn

% COLUMN 3
\begin{column}{\colwidth}
\begin{exampleblock}{Theoretical Foundations}
    The mathematical foundation of deep learning involves optimization theory and linear algebra. The learning process minimizes the loss function:
    \begin{equation}
        \min_\theta \frac{1}{N} \sum_{i=1}^N \mathcal{L}(f_\theta(x_i), y_i) + \lambda R(\theta)
    \end{equation}
    where $\mathcal{L}$ is the loss function, $R(\theta)$ is the regularization term, and $\lambda$ controls regularization strength.
    
    \begin{center}
    \begin{tikzpicture}
        \begin{axis}[
            width=14cm,
            height=7cm,
            xmin=0, xmax=100,
            ymin=0, ymax=2.5,
            axis lines=middle,
            xlabel={Training Epochs},
            ylabel={Loss Value},
            samples=100,
            grid=both,
        ]
        
        % Training loss curve
        \addplot[domain=0:100, samples=100, thick, color=blue]
        {2.0 * exp(-0.05*x) + 0.1};
        
        % Validation loss curve
        \addplot[domain=0:100, samples=100, thick, color=red, dashed]
        {1.8 * exp(-0.04*x) + 0.15};
        
        \legend{Training Loss, Validation Loss}
        
        \end{axis}
    \end{tikzpicture}
    \end{center}
    This visualization shows typical learning curves where both training and validation losses decrease over epochs, indicating successful model convergence.
\end{exampleblock}

\begin{exampleblock}{Conclusion and Future Work}
\begin{itemize}
  \item Deep learning models demonstrate superior performance across various tasks
  \item Transformer architectures excel in natural language processing
  \item Continued research needed for explainable and trustworthy AI
  \item Future directions include multimodal learning and AI safety
\end{itemize}
\end{exampleblock}

\begin{block}{References}
\small
\footnotesize{\bibliographystyle{plain}\bibliography{ref}}
\end{block}
\end{column}

\end{columns}
\end{frame}
\end{document}
